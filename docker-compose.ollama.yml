
version: "3.9"

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped

    # Exponha a API do Ollama (HTTP) na porta padrão 11434
    ports:
      - "11434:11434"

    # Persistência do cache/modelos do Ollama
    volumes:
      - ollama_data:/root/.ollama

    # Variáveis úteis do servidor
    environment:
      # Servir em todas as interfaces (necessário se outro container ou máquina vai consumir)
      - OLLAMA_HOST=0.0.0.0
      # Mantém modelos carregados em memória por mais tempo (ajuste conforme RAM)
      - OLLAMA_KEEP_ALIVE=24h
      # Quantos modelos simultâneos podem ficar carregados (otimize conforme RAM)
      - OLLAMA_MAX_LOADED_MODELS=2
      # Quantas requisições o servidor processa em paralelo (ajuste com cuidado)
      - OLLAMA_NUM_PARALLEL=1
      # (Opcional) Libere CORS se um frontend direto for chamar o servidor Ollama
      # - OLLAMA_ORIGINS=*

    # (Opcional) habilitar GPU NVIDIA — requer Docker com suporte a GPUs
    # Comente/Descomente conforme seu host:
    # gpus: all

    # (Opcional) Aumente shm se estiver carregando modelos grandes
    # shm_size: "2g"

    # Saúde: só fica "healthy" quando a API responde
    healthcheck:
      # 'ollama list' conversa com o servidor; falha se não estiver pronto
      test: ["CMD-SHELL", "ollama list >/dev/null 2>&1 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 10s

volumes:
  ollama_data:
