from langchain.embeddings import OllamaEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.llms import Ollama
from app.biblia_api import buscar_versiculo

llm = Ollama(model="gemma:2b")
embedding_model = OllamaEmbeddings(model="gemma:2b")
db = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embedding_model
)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=db.as_retriever(),
    return_source_documents=True
)


def responder_pergunta_com_versiculo(pergunta: str) -> str:
    resultado = qa_chain.invoke(pergunta)
    resposta = resultado.get('result', '')
    if "salvaÃ§Ã£o" in pergunta.lower():
        try:
            versiculo = buscar_versiculo("JoÃ£o 3:16")
            if versiculo:
                resposta += f"\nğŸ“– JoÃ£o 3:16 â€” {versiculo}"
            else:
                resposta += "\nğŸ“– JoÃ£o 3:16 â€” VersÃ­culo nÃ£o encontrado."
        except Exception as e:
            resposta += f"\nErro ao buscar versÃ­culo: {e}"
    return resposta
