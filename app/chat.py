from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain_community.llms import Ollama


llm = Ollama(model="gemma:2b")
embedding_model = OllamaEmbeddings(model="gemma:2b")
db = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embedding_model
)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=db.as_retriever(),
    return_source_documents=True
)


def buscar_versiculo(referencia: str) -> str:
    # Exemplo simples de busca de versÃ­culo (substitua por lÃ³gica real
    # conforme necessÃ¡rio)
    versiculos = {
        "JoÃ£o 3:16": (
            "Porque Deus amou o mundo de tal maneira que deu o seu Filho "
            "unigÃªnito, para que todo aquele que nele crÃª nÃ£o pereÃ§a, mas "
            "tenha a vida eterna."
        )
    }
    return versiculos.get(referencia, "")


def responder_pergunta_com_versiculo(pergunta: str) -> str:
    resultado = qa_chain.invoke(pergunta)
    resposta = resultado.get('result', '')
    if "salvaÃ§Ã£o" in pergunta.lower():
        try:
            versiculo = buscar_versiculo("JoÃ£o 3:16")
            if versiculo:
                resposta += f"\nğŸ“– JoÃ£o 3:16 â€” {versiculo}"
            else:
                resposta += "\nğŸ“– JoÃ£o 3:16 â€” VersÃ­culo nÃ£o encontrado."
        except Exception as e:
            resposta += f"\nErro ao buscar versÃ­culo: {e}"
    return resposta
